#we consider with alpha = 0.05
#We use : CI = [p(mean) - fract(0.975*s/sqrt(n)), p(mean) + fract(0.975*s/sqrt(n))]
ptrain <- as.matrix(average_error_rate_kppv[, 1]);
ptest <- as.matrix(average_error_rate_kppv[, 2]);
frac = 1.96;
CI_tab <- matrix(0, nrow = nrow(average_error_rate_kppv), ncol = 4); #4 because of two values for each interval
for (i in 1:nrow(average_error_rate)){
CI_tab[i, 1] <- ptrain[i] - frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on train set
CI_tab[i, 2] <- ptrain[i] + frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on train set
CI_tab[i, 3] <- ptest[i] - frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on test set
CI_tab[i, 4] <- ptest[i] + frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on test set
}
average_error_rate_kppv
length(zapp[which(as.matrix(zapp) != as.matrix(data.sep$zapp))]) / dim(data.sep$Xapp)[1];
#AT FIRST : USE load-data.R file
#list of our datasets we want to estimate
donn_list = list(donn40, donn100, donn500, donn1000);
#error matrix
error_train_kppv <- matrix(0, length(donn_list), 20);
error_test_kppv <-  matrix(0, length(donn_list), 20);
#start of algorithm
for (i in 1:length(donn_list)) {
data <- as.data.frame(donn_list[])
X <- data[,1:2];
z <- data[,3];
for (j in 1:20 ){
donn.sep <- separ2(X, z) #using separ1.R function
#parting in learning and test sets
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xval <- donn.sep$Xval
zval <- donn.sep$zval
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
#we estimate parameters on Xapp
Kopt <- kppv.tune(Xapp, zapp, Xval, zval,2*(1:6)-1)
#applying values on individuals of sets
zapp_val <- kppv.val(Xapp, zapp, Kopt, Xapp);
ztst_val <- kppv.val(Xapp, zapp, Kopt, Xtst);
#error rate
error_train_kppv[i,j] <- length(zapp[which(as.matrix(zapp) != as.matrix(zapp))]) / dim(Xapp)[1];
error_test_kppv[i,j] <- length(ztst[which(as.matrix(ztst) != as.matrix(ztst))]) / dim(Xtst)[1];
}
}
#applying mean on rows which represent each dataset
average_error_rate_kppv <- matrix(0, nrow = length(donn_list), ncol = 2);
average_error_rate_kppv[,1] = apply(error_train_kppv, 1, mean);
average_error_rate_kppv[,2] = apply(error_test_kppv, 1, mean);
#Confidence interval
#we consider with alpha = 0.05
#We use : CI = [p(mean) - fract(0.975*s/sqrt(n)), p(mean) + fract(0.975*s/sqrt(n))]
ptrain <- as.matrix(average_error_rate_kppv[, 1]);
ptest <- as.matrix(average_error_rate_kppv[, 2]);
frac = 1.96;
CI_tab <- matrix(0, nrow = nrow(average_error_rate_kppv), ncol = 4); #4 because of two values for each interval
for (i in 1:nrow(average_error_rate)){
CI_tab[i, 1] <- ptrain[i] - frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on train set
CI_tab[i, 2] <- ptrain[i] + frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on train set
CI_tab[i, 3] <- ptest[i] - frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on test set
CI_tab[i, 4] <- ptest[i] + frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on test set
}
average_error_rate_kppv
#AT FIRST : USE load-data.R file
#list of our datasets we want to estimate
donn_list = list(donn40, donn100, donn500, donn1000);
#error matrix
error_train_kppv <- matrix(0, length(donn_list), 20);
error_test_kppv <-  matrix(0, length(donn_list), 20);
#start of algorithm
for (i in 1:length(donn_list)) {
data <- as.data.frame(donn_list[])
X <- data[,1:2];
z <- data[,3];
for (j in 1:20 ){
donn.sep <- separ2(X, z) #using separ1.R function
#parting in learning and test sets
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xval <- donn.sep$Xval
zval <- donn.sep$zval
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
#we estimate parameters on Xapp
Kopt <- kppv.tune(Xapp, zapp, Xval, zval,2*(1:6)-1);
print(Kopt);
#applying values on individuals of sets
zapp_val <- kppv.val(Xapp, zapp, Kopt, Xapp);
ztst_val <- kppv.val(Xapp, zapp, Kopt, Xtst);
#error rate
error_train_kppv[i,j] <- length(zapp[which(as.matrix(zapp) != as.matrix(zapp))]) / dim(Xapp)[1];
error_test_kppv[i,j] <- length(ztst[which(as.matrix(ztst) != as.matrix(ztst))]) / dim(Xtst)[1];
}
}
#applying mean on rows which represent each dataset
average_error_rate_kppv <- matrix(0, nrow = length(donn_list), ncol = 2);
average_error_rate_kppv[,1] = apply(error_train_kppv, 1, mean);
average_error_rate_kppv[,2] = apply(error_test_kppv, 1, mean);
#Confidence interval
#we consider with alpha = 0.05
#We use : CI = [p(mean) - fract(0.975*s/sqrt(n)), p(mean) + fract(0.975*s/sqrt(n))]
ptrain <- as.matrix(average_error_rate_kppv[, 1]);
ptest <- as.matrix(average_error_rate_kppv[, 2]);
frac = 1.96;
CI_tab <- matrix(0, nrow = nrow(average_error_rate_kppv), ncol = 4); #4 because of two values for each interval
for (i in 1:nrow(average_error_rate)){
CI_tab[i, 1] <- ptrain[i] - frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on train set
CI_tab[i, 2] <- ptrain[i] + frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on train set
CI_tab[i, 3] <- ptest[i] - frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on test set
CI_tab[i, 4] <- ptest[i] + frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on test set
}
kppv.tune <- function(xapp, zapp, Xval, zval, nppv){
length_nppv <- length(nppv);
listError <- vector("numeric", length_nppv);
for (i in nppv){
min <- i;
erreur_opt <- 1
result <- kppv.val(xapp, zapp, i, Xval);
erreur <- sum((result == zval) == TRUE)/length(zval)
if (erreur_opt > erreur){
erreur_opt <- erreur;
min <- i
}
}
return (min);
}
kppv.tune = function(Xapp, zapp, Xval, zval, nppv) {
# determines the optimal neighbours number to take into account
# to determine the individual's class
# IN:
# - Xapp : the training individuals matrix
# - zapp : the labels of the training individuals
# - Xval : the validation individuals matrix (used to test the optimal value
# of K)
# - zval : the validation individual labels
# - nppv : the different values of K to test
# OUT:
# - Kopt : value of the optimal K
# need library "flexclust" for dist2
# we use this lib instead of the distXY function because we had some
# problems using distXY at first because we were not passing arguments
# casted into the right type (we figured this out afterward)
library(flexclust);
# K values matrix
Kopt = matrix(nrow=dim(as.matrix(nppv))[1], ncol=1);
# pour chaque valeur de K
for (i in 1:length(nppv)) {
# pour chaque individu de X val
ztst = kppv.val(Xapp, zapp, nppv[i], Xval);
# calcul du taux d'erreur
Kopt[i,] = (length(ztst[which(as.matrix(ztst) != as.matrix(zval))]) / length(zval) * 100);
}
return (nppv[which.min(Kopt)]);
}
#AT FIRST : USE load-data.R file
#list of our datasets we want to estimate
donn_list = list(donn40, donn100, donn500, donn1000);
#error matrix
error_train_kppv <- matrix(0, length(donn_list), 20);
error_test_kppv <-  matrix(0, length(donn_list), 20);
#start of algorithm
for (i in 1:length(donn_list)) {
data <- as.data.frame(donn_list[])
X <- data[,1:2];
z <- data[,3];
for (j in 1:20 ){
donn.sep <- separ2(X, z) #using separ1.R function
#parting in learning and test sets
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xval <- donn.sep$Xval
zval <- donn.sep$zval
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
#we estimate parameters on Xapp
Kopt <- kppv.tune(Xapp, zapp, Xval, zval,2*(1:6)-1);
print(Kopt);
#applying values on individuals of sets
zapp_val <- kppv.val(Xapp, zapp, Kopt, Xapp);
ztst_val <- kppv.val(Xapp, zapp, Kopt, Xtst);
#error rate
error_train_kppv[i,j] <- length(zapp[which(as.matrix(zapp) != as.matrix(zapp))]) / dim(Xapp)[1];
error_test_kppv[i,j] <- length(ztst[which(as.matrix(ztst) != as.matrix(ztst))]) / dim(Xtst)[1];
}
}
#applying mean on rows which represent each dataset
average_error_rate_kppv <- matrix(0, nrow = length(donn_list), ncol = 2);
average_error_rate_kppv[,1] = apply(error_train_kppv, 1, mean);
average_error_rate_kppv[,2] = apply(error_test_kppv, 1, mean);
#Confidence interval
#we consider with alpha = 0.05
#We use : CI = [p(mean) - fract(0.975*s/sqrt(n)), p(mean) + fract(0.975*s/sqrt(n))]
ptrain <- as.matrix(average_error_rate_kppv[, 1]);
ptest <- as.matrix(average_error_rate_kppv[, 2]);
frac = 1.96;
CI_tab <- matrix(0, nrow = nrow(average_error_rate_kppv), ncol = 4); #4 because of two values for each interval
for (i in 1:nrow(average_error_rate)){
CI_tab[i, 1] <- ptrain[i] - frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on train set
CI_tab[i, 2] <- ptrain[i] + frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on train set
CI_tab[i, 3] <- ptest[i] - frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on test set
CI_tab[i, 4] <- ptest[i] + frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on test set
}
kppv.tune <- function(xapp, zapp, Xval, zval, nppv){
length_nppv <- length(nppv);
listError <- vector("numeric", length_nppv);
for (i in nppv){
min <- i;
erreur_opt <- 1
result <- kppv.val(xapp, zapp, i, Xval);
erreur <- sum((result == zval) == TRUE)/length(zval)
if (erreur_opt > erreur){
erreur_opt <- erreur;
min <- i
}
}
return (min);
}
kppv.val <- function(Xapp, zapp, K, Xtst)
{
etiquette <- vector(length = nrow(Xtst))
distanceXtstXapp <- matrix(ncol = nrow(Xtst), nrow = nrow(Xapp))
distanceXtstXapp <- distXY(Xapp,Xtst)
sort_table <- apply(distanceXtstXapp, 2, order); #le 2 signifie qu'on fait le trie sur les colonnes
sort_table2 <- sort_table;
for(i in 1:K){
for(j in 1:ncol(sort_table2)){
sort_table2[i,j] <- zapp[sort_table[i,j]];
}
}
sort_table2 <- sort_table2[1:K,];
if (is.null(dim(sort_table2))){
return (sort_table2);
}
else return(round(apply(sort_table2, 2, mean)));
}
front.kppv <- function(Xapp, zapp, K, discretisation=50)
{
deltaX <- (max(X[,1]) -min(X[,1]))/discretisation
deltaY <- (max(X[,2]) -min(X[,2]))/discretisation
minX <- min(X[,1])-deltaX
maxX <- max(X[,1])+deltaX
minY <- min(X[,2])-deltaY
maxY <- max(X[,2])+deltaY
# grille d'affichage
grilleX <- seq(from=minX,to=maxX,by=deltaX)
naffX <- length(grilleX)
grilleY <- seq(from=minY,to=maxY,by=deltaY)
naffY <- length(grilleY)
grille <- cbind(rep.int(grilleX,times=rep(naffY,naffX)),rep(grilleY,naffX))
# calcul des valeurs de la fonction
valf <- kppv.val(Xapp, zapp, K, grille)
plot(Xapp, col=c("red","green","blue","magenta","orange")[zapp], asp=1)
contour(grilleX, grilleY, matrix(valf,nrow=naffX,byrow=T), add=T, drawlabels=FALSE, levels=1.5)
}
kppv.val <- function(Xapp, zapp, K, Xtst)
{
etiquette <- vector(length = nrow(Xtst))
for (i in 1:nrow(Xtst)){
tab_voisins <- vector("numeric",length = K);
distanceXtstXapp <- distXY(Xtst[i,], Xapp)
sort_table <- sort(distanceXtstXapp,index.return = TRUE)$ix;
for(l in 1:K){
tab_voisins[l] = zapp[sort_table[l]];
}
etiquette[i] <- unique(which.max(table(tab_voisins)));
}
return(etiquette);
}
X <- donn[,1:2]
z <- donn[,3]
front.kppv(X, z, 3, 500)
distXY(Xtst[1,], Xapp)
a <- distXY(Xtst[1,], Xapp)
a
kppv.val <- function(Xapp, zapp, K, Xtst)
{
etiquette <- vector(length = nrow(Xtst))
for (i in 1:nrow(Xtst)){
tab_voisins <- vector("numeric",length = K);
distanceXtstXapp <- distXY(Xtst[1,], Xapp)
sort_table <- sort(distanceXtstXapp,index.return = TRUE)$ix;
for(l in 1:K){
tab_voisins[l] = zapp[sort_table[l]];
}
etiquette[i] <- unique(which.max(table(tab_voisins)));
}
return(etiquette);
}
front.kppv(X, z, 3, 500)
kppv.val <- function(Xapp, zapp, K, Xtst)
{
etiquette <- vector(length = nrow(Xtst))
distanceXtstXapp <- matrix(ncol = nrow(Xtst), nrow = nrow(Xapp))
distanceXtstXapp <- distXY(Xapp,Xtst)
sort_table <- apply(distanceXtstXapp, 2, order); #le 2 signifie qu'on fait le trie sur les colonnes
sort_table2 <- sort_table;
for(i in 1:K){
for(j in 1:ncol(sort_table2)){
sort_table2[i,j] <- zapp[sort_table[i,j]];
}
}
sort_table2 <- sort_table2[1:K,];
if (is.null(dim(sort_table2))){
return (sort_table2);
}
else return(round(apply(sort_table2, 2, mean)));
}
front.kppv(X, z, 3, 500)
#AT FIRST : USE load-data.R file
#list of our datasets we want to estimate
donn_list = list(donn40, donn100, donn500, donn1000);
#error matrix
error_train_kppv <- matrix(0, length(donn_list), 20);
error_test_kppv <-  matrix(0, length(donn_list), 20);
#start of algorithm
for (i in 1:length(donn_list)) {
data <- as.data.frame(donn_list[])
X <- data[,1:2];
z <- data[,3];
for (j in 1:20 ){
donn.sep <- separ2(X, z) #using separ1.R function
#parting in learning and test sets
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xval <- donn.sep$Xval
zval <- donn.sep$zval
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
#we estimate parameters on Xapp
Kopt <- kppv.tune(Xapp, zapp, Xval, zval,2*(1:6)-1);
print(Kopt);
#applying values on individuals of sets
zapp_val <- kppv.val(Xapp, zapp, Kopt, Xapp);
ztst_val <- kppv.val(Xapp, zapp, Kopt, Xtst);
#error rate
error_train_kppv[i,j] <- length(zapp[which(as.matrix(zapp) != as.matrix(zapp))]) / dim(Xapp)[1];
error_test_kppv[i,j] <- length(ztst[which(as.matrix(ztst) != as.matrix(ztst))]) / dim(Xtst)[1];
}
}
#applying mean on rows which represent each dataset
average_error_rate_kppv <- matrix(0, nrow = length(donn_list), ncol = 2);
average_error_rate_kppv[,1] = apply(error_train_kppv, 1, mean);
average_error_rate_kppv[,2] = apply(error_test_kppv, 1, mean);
#Confidence interval
#we consider with alpha = 0.05
#We use : CI = [p(mean) - fract(0.975*s/sqrt(n)), p(mean) + fract(0.975*s/sqrt(n))]
ptrain <- as.matrix(average_error_rate_kppv[, 1]);
ptest <- as.matrix(average_error_rate_kppv[, 2]);
frac = 1.96;
CI_tab <- matrix(0, nrow = nrow(average_error_rate_kppv), ncol = 4); #4 because of two values for each interval
for (i in 1:nrow(average_error_rate)){
CI_tab[i, 1] <- ptrain[i] - frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on train set
CI_tab[i, 2] <- ptrain[i] + frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on train set
CI_tab[i, 3] <- ptest[i] - frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on test set
CI_tab[i, 4] <- ptest[i] + frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on test set
}
average_error_rate_kppv
CI_tab
#AT FIRST : USE load-data.R file
#list of our datasets we want to estimate
donn_list = list(donn40, donn100, donn500, donn1000);
opti_neigh = matrix(0, nrow = length(donn_list), ncol = 1)
for (i in 1:length(donn_list)) {
data <- as.data.frame(donn_list[])
X <- data[,1:2];
z <- data[,3];
donn.sep <- separ1(X, z) #using separ1.R function
#parting in learning and test sets
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
opti_neigh[i] <- kppv.tune(Xapp, zapp, Xapp, zapp, 2*(1:6)-1)
}
CI_tab
opti_neigh
#AT FIRST : USE load-data.R file
#list of our datasets we want to estimate
donn_list = list(donn40, donn100, donn500, donn1000);
#error matrix
error_train <- matrix(0, length(donn_list), 20);
error_test <-  matrix(0, length(donn_list), 20);
#start of algorithm
for (i in 1:length(donn_list)) {
data <- as.data.frame(donn_list[])
X <- data[,1:2];
z <- data[,3];
for (j in 1:20 ){
donn.sep <- separ1(X, z) #using separ1.R function
#parting in learning and test sets
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
#we estimate parameters on Xapp
mu <- ceuc.app(Xapp, zapp);
#applying values on individuals of sets
train_set <- ceuc.val(Xapp, mu);
test_set <- ceuc.val(Xtst, mu);
#error rate
error_train[i,j] <- dim(as.matrix(which(train_set != as.matrix(zapp))))[1] / dim(as.matrix(zapp))[1];
error_test[i,j] <- dim(as.matrix(which(test_set != as.matrix(ztst))))[1] / dim(as.matrix(zapp))[1];
}
}
#applying mean on rows which represent each dataset
average_error_rate <- matrix(0, nrow = length(donn_list), ncol = 2);
average_error_rate[,1] = apply(error_train, 1, mean);
average_error_rate[,2] = apply(error_test, 1, mean);
#Confidence interval
#we consider with alpha = 0.05
#We use : CI = [p(mean) - fract(0.975*s/sqrt(n)), p(mean) + fract(0.975*s/sqrt(n))]
ptrain <- as.matrix(average_error_rate[, 1]);
ptest <- as.matrix(average_error_rate[, 2]);
frac = 1.96;
CI_tab <- matrix(0, nrow = nrow(average_error_rate), ncol = 4); #4 because of two values for each interval
for (i in 1:nrow(average_error_rate)){
CI_tab[i, 1] <- ptrain[i] - frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on train set
CI_tab[i, 2] <- ptrain[i] + frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on train set
CI_tab[i, 3] <- ptest[i] - frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on test set
CI_tab[i, 4] <- ptest[i] + frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on test set
}
CI_tab
#1er Jeu
donn40 <- read.csv("Synth1-40.csv")
X40 <- donn40[,1:2]
z40 <- donn40[,3]
donn100 <- read.csv("Synth1-100.csv")
X100 <- donn100[,1:2]
z100 <- donn100[,3]
donn500 <- read.csv("Synth1-500.csv")
X500 <- donn500[,1:2]
z500 <- donn500[,3]
donn1000 <- read.csv("Synth1-1000.csv")
X1000 <- donn1000[,1:2]
z1000 <- donn1000[,3]
donn1000_2 <- read.csv("Synth2-1000.csv")
X1000_2 <- donn1000[,1:2]
z1000_2 <- donn1000[,3]
dataPima <- read.csv("Pima.csv")
View(dataPima)
dataPima <- read.csv("Pima.csv")
XPim <- dataPima[,1:7]
zPim <- dataPima[,8]
data = as.data.frame(dataPima);
mu1 = apply(data[which(data$z == 1), 1:2], 2, mean);
mu2 = apply(data[which(data$z == 2), 1:2], 2, mean);
epsilon1 = cov(data[which(data$z == 1), 1:2]);
epsilon2 = cov(data[which(data$z == 2), 1:2]);
pi1 = dim(data[which(data$z == 1),])[1] / dim(data)[1];
pi2 = dim(data[which(data$z == 2),])[1] / dim(data)[1];
df = data.frame("mu1" = mu1, "mu2" = mu2, "epsilon1" = epsilon1, "epsilon2" = epsilon2, "pi1" = pi1, "pi2" = pi2);
write.table(df, paste("Pima","_analysis.csv", sep=""), sep = ';'); #do not forget "sep" arg in ordre to define columns
#load file
dataPima <- read.csv("Pima.csv")
XPim <- dataPima[,1:7]
zPim <- dataPima[,8]
#/!\--------Estimate parameters---------/!\
data = as.data.frame(dataPima);
mu1 = apply(data[which(data$z == 1), 1:2], 2, mean);
mu2 = apply(data[which(data$z == 2), 1:2], 2, mean);
epsilon1 = cov(data[which(data$z == 1), 1:2]);
epsilon2 = cov(data[which(data$z == 2), 1:2]);
pi1 = dim(data[which(data$z == 1),])[1] / dim(data)[1];
pi2 = dim(data[which(data$z == 2),])[1] / dim(data)[1];
df = data.frame("mu1" = mu1, "mu2" = mu2, "epsilon1" = epsilon1, "epsilon2" = epsilon2, "pi1" = pi1, "pi2" = pi2);
write.table(df, paste("Pima","_analysis.csv", sep=""), sep = ';'); #do not forget "sep" arg in ordre to define columns
#/!\-------EUCLIDEAN CLASSIFIER ANALYSIS---------/!\
#Divide data
donn.sep <- separ1(XPim, zPim)
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
#error matrix
error_train <- matrix(0, 1, 20);
error_test <-  matrix(0, 1, 20);
#start of algorithm
for (j in 1:20 ){
donn.sep <- separ1(XPim, zPim) #using separ1.R function
#parting in learning and test sets
Xapp <- donn.sep$Xapp
zapp <- donn.sep$zapp
Xtst <- donn.sep$Xtst
ztst <- donn.sep$ztst
#we estimate parameters on Xapp
mu <- ceuc.app(Xapp, zapp);
#applying values on individuals of sets
train_set <- ceuc.val(Xapp, mu);
test_set <- ceuc.val(Xtst, mu);
#error rate
error_train[1,j] <- dim(as.matrix(which(train_set != as.matrix(zapp))))[1] / dim(as.matrix(zapp))[1];
error_test[1,j] <- dim(as.matrix(which(test_set != as.matrix(ztst))))[1] / dim(as.matrix(zapp))[1];
}
}
#applying mean on rows which represent each dataset
average_error_rate_Pima <- matrix(0, nrow = 1, ncol = 2);
average_error_rate_Pima[,1] = apply(error_train, 1, mean);
average_error_rate_Pima[,2] = apply(error_test, 1, mean);
#Confidence interval
#we consider with alpha = 0.05
#We use : CI = [p(mean) - fract(0.975*s/sqrt(n)), p(mean) + fract(0.975*s/sqrt(n))]
ptrain <- as.matrix(average_error_rate_Pima[, 1]);
ptest <- as.matrix(average_error_rate_Pima[, 2]);
frac = 1.96;
CI_Pima <- matrix(0, nrow = nrow(average_error_rate_Pima), ncol = 4); #4 because of two values for each interval
for (i in 1:nrow(average_error_rate_Pima)){
CI_Pima[i, 1] <- ptrain[i] - frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on train set
CI_Pima[i, 2] <- ptrain[i] + frac * sqrt((ptrain[i]*(1-ptrain[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on train set
CI_Pima[i, 3] <- ptest[i] - frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #lower bound for CI on test set
CI_Pima[i, 4] <- ptest[i] + frac * sqrt((ptest[i]*(1-ptest[i]))/nrow(as.data.frame(donn_list[i]))); #upper bound for CI on test set
}
average_error_rate_Pima
CI_Pima
